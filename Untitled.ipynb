{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0efd9d6-e55e-404d-bd9f-5194b1c0d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,      # Class for causal models (GPT, Llama, etc.)\n",
    "    AutoTokenizer,              # Automatic tokenizer\n",
    "    TrainingArguments,          # Training configuration\n",
    "    Trainer,                    # HuggingFace trainer class\n",
    "    DataCollatorForLanguageModeling  # Prepares batches for language modeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,                 # LoRA configuration\n",
    "    get_peft_model,             # Applies LoRA to model\n",
    "    TaskType                    # Task type (CAUSAL_LM, SEQ_2_SEQ, etc.)\n",
    ")\n",
    "from datasets import load_dataset  # Load datasets from HuggingFace Hub\n",
    "import os\n",
    "\n",
    "FINE_TUNED_MODEL_NAME = os.getenv(\"FINE_TUNED_MODEL_NAME\", \"\")\n",
    "\n",
    "# Check if MPS is available\n",
    "mps_available = torch.backends.mps.is_available()\n",
    "mps_built = torch.backends.mps.is_built()\n",
    "\n",
    "# Select the best available device\n",
    "if mps_available:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9915a630-a377-4816-b502-06930d8355d3",
   "metadata": {},
   "source": [
    "2. Load Model and Tokenizer\n",
    "For this guide, weâ€™ll use a lightweight ~1B model: Llama-3.2â€“1B-Instruct. If the model is gated, log into Hugging Face and accept the license (huggingface-cli login)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8df35d2-3590-4fcd-9de6-2543d9e808e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model to fine-tune\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# The tokenizer converts text to token IDs and vice versa\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True  # Allows custom code (required for some models)\n",
    ")\n",
    "\n",
    "# This is necessary for batching during training\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float32,  # Use reduced precision to save memory\n",
    "    device_map={\"\": device},     # Map model to selected device\n",
    "    trust_remote_code=True       # Allow custom code\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa236941-180f-49d1-b338-ef21c23afb72",
   "metadata": {},
   "source": [
    "Start with small models (0.5â€“1B) to validate your pipeline\n",
    "For 7B models on 16GB unified memory: use batch_size=1, high gradient accumulation, LoRA with r=8â€“16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed22e040-d3e9-4173-baaa-716c98ddc128",
   "metadata": {},
   "source": [
    "3. Inspect Model Structure (Recommended)\n",
    "Identify linear layers where LoRA should be applied (typically attention projections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f8042e-f4af-4676-bc19-e2db8d7efec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Linear modules found: {'k_proj', 'up_proj', 'gate_proj', 'v_proj', 'o_proj', 'lm_head', 'q_proj', 'down_proj'}\n",
      "âœ… Recommended modules for LoRA: {'q_proj', 'k_proj', 'up_proj', 'gate_proj', 'v_proj', 'o_proj', 'down_proj'}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "linear_modules = set()\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        linear_modules.add(name.split('.')[-1])\n",
    "\n",
    "exclude_modules = {'lm_head', 'embed_tokens', 'wte', 'wpe', 'ln_f'}\n",
    "#why those exclusions\n",
    "recommended_modules = linear_modules - exclude_modules\n",
    "print(f\"\\nðŸ“‹ Linear modules found: {linear_modules}\")\n",
    "print(f\"âœ… Recommended modules for LoRA: {recommended_modules}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c0815d-4a98-48ba-8748-78f69a549a91",
   "metadata": {},
   "source": [
    "4. Configure and Apply LoRA\n",
    "Weâ€™ll apply LoRA to train only the adapters while keeping the base model frozen.\n",
    "\n",
    "LoRA parameters explained:\n",
    "\n",
    "r (rank): Dimension of LoRA matrices. Typical values: 8-64. Higher = more capacity but more parameters\n",
    "lora_alpha: Scaling factor. Typically 2Ã—r. Controls adapter influence\n",
    "target_modules: Which layers to modify (attention, projections, etc.)\n",
    "lora_dropout: Dropout for regularization (0.05-0.1)\n",
    "bias: Whether to train biases (\"none\", \"all\", \"lora_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03492a01-7712-4bd8-a5a2-e191dc6e44bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target modules: ['q_proj', 'k_proj', 'up_proj', 'gate_proj', 'v_proj', 'o_proj', 'down_proj']\n"
     ]
    }
   ],
   "source": [
    "def find_target_modules(model, exclude_names=None):\n",
    "    exclude_names = exclude_names or {'lm_head', 'embed_tokens', 'wte', 'wpe', 'ln_f'}\n",
    "    target_modules = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            target_modules.add(name.split('.')[-1])\n",
    "    return list(target_modules - exclude_names)\n",
    "\n",
    "target_modules = find_target_modules(model)\n",
    "print(f\"Target modules: {target_modules}\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabec1bb-7744-474d-a601-9630801f630a",
   "metadata": {},
   "source": [
    "5. Dataset Preparation\n",
    "Weâ€™ll use IMDB (1000-sample subset) for quick testing. You can substitute any HuggingFace dataset.\n",
    "\n",
    "What weâ€™re doing:\n",
    "\n",
    "Load dataset from HuggingFace Hub\n",
    "Tokenize the text (convert to token IDs)\n",
    "Apply padding and truncation to normalize lengths\n",
    "Split into train/validation sets\n",
    "\n",
    "Other useful testing datasets:\n",
    "\n",
    "wikitext: Wikipedia articles\n",
    "openwebtext: Web text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a8921e-1d82-4215-a9b9-3907ac607560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 900 | Eval: 100\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\", split=\"train[:1000]\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=list(dataset.column_names),\n",
    "    desc=\"Tokenizing dataset\"\n",
    ")\n",
    "\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset, eval_dataset = split[\"train\"], split[\"test\"]\n",
    "print(f\"Train: {len(train_dataset)} | Eval: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f829ed-f186-4f1a-910a-f61f1b451b5d",
   "metadata": {},
   "source": [
    "6. Training Configuration\n",
    "Now weâ€™ll configure all training parameters. Here are the key ones explained:\n",
    "\n",
    "Enter your email\n",
    "Subscribe\n",
    "Batch Size and Gradient Accumulation:\n",
    "\n",
    "per_device_train_batch_size: How many examples to process together (higher = faster but more memory)\n",
    "gradient_accumulation_steps: Accumulate gradients for N steps before updating weights (simulates larger batch sizes)\n",
    "Effective batch size = per_device_train_batch_size Ã— gradient_accumulation_steps\n",
    "Learning Rate:\n",
    "\n",
    "learning_rate: How quickly the model learns (2e-4 is a solid default for LoRA)\n",
    "Strategies:\n",
    "\n",
    "evaluation_strategy: When to evaluate the model (\"steps\", \"epoch\", \"no\")\n",
    "save_strategy: When to save checkpoints (\"steps\", \"epoch\", \"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8164827-5acf-4f6a-84c8-9f2f2a066d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./my-finetuned-model\",  # use a descriptive name\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,      # Reduced for MPS stability\n",
    "    per_device_eval_batch_size=2,       # Reduced for MPS stability\n",
    "    gradient_accumulation_steps=8,      # Increased to maintain effective batch size\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=100,\n",
    "\n",
    "    gradient_checkpointing=False,       # Disable for MPS compatibility\n",
    "\n",
    "    fp16=False,                         # MPS: keep fp32\n",
    "    bf16=False,\n",
    "\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    remove_unused_columns=False,\n",
    "    seed=42,\n",
    "    \n",
    "    # MPS specific optimizations\n",
    "    dataloader_pin_memory=False,        # Disable pin memory for MPS\n",
    "    torch_compile=False,                # Disable torch compile for MPS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe1b916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared with labels for causal language modeling\n"
     ]
    }
   ],
   "source": [
    "# Fix dataset to ensure labels are properly set for causal language modeling\n",
    "def prepare_dataset(examples):\n",
    "    # For causal language modeling, labels should be the same as input_ids\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "train_dataset = train_dataset.map(prepare_dataset, batched=False)\n",
    "eval_dataset = eval_dataset.map(prepare_dataset, batched=False)\n",
    "\n",
    "print(\"Dataset prepared with labels for causal language modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58917a11-fb61-4a2b-b1d1-a104a16fe4cd",
   "metadata": {},
   "source": [
    "7. Start Training\n",
    "Initialize the HuggingFace Trainer and launch the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f37d05a-1a44-4bb7-a618-fb5937b6d422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/171 16:59 < 47:34:35, 0.00 it/s, Epoch 0.04/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"â€¢ Eval loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"â€¢ Perplexity: {np.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ca15b-719b-4770-a4f4-397e04cbfc13",
   "metadata": {},
   "source": [
    "What happens during training:\n",
    "\n",
    "The model processes data batches\n",
    "Calculates loss (prediction error)\n",
    "Computes gradients via backpropagation\n",
    "Updates only LoRA parameters (not the entire model)\n",
    "Periodically evaluates on the validation set\n",
    "Saves checkpoints\n",
    "Critical metrics to monitor:\n",
    "\n",
    "loss: Training set error (should decrease)\n",
    "eval_loss: Validation set error (should decrease, but not too much relative to loss)\n",
    "learning_rate: Changes during warmup phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc3033-2d60-44e9-9320-235fc66fb467",
   "metadata": {},
   "source": [
    "Save LoRA Adapters and Tokenizer\n",
    "Training is complete. Now letâ€™s save the model to disk. LoRA adapters are extremely lightweight â€” you can save multiple versions with different names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e182da-7cd5-46bf-9c37-03c4f4476c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "FINE_TUNED_MODEL_NAME = \"llama-1b-imdb-lora\"  # alternatively use ENV\n",
    "output_dir = Path(f\"./{FINE_TUNED_MODEL_NAME}-finetuned\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(output_dir)    # save LoRA adapters\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"âœ“ Adapters and tokenizer saved to:\", output_dir)\n",
    "for f in output_dir.iterdir():\n",
    "    print(\"  â€¢\", f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bd16ea-8e65-41cf-a73d-7f0f7a31355b",
   "metadata": {},
   "source": [
    "To reload later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fac83f-afcd-4350-8179-4ff132013979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32).to(device)\n",
    "model = PeftModel.from_pretrained(base_model, str(output_dir))\n",
    "tokenizer = AutoTokenizer.from_pretrained(str(output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d1ac3e-a0ea-4499-8a0a-c38e21b14a7f",
   "metadata": {},
   "source": [
    "If you want a standalone model thatâ€™s easier to deploy, merge the adapters (the model will be larger on disk and you wonâ€™t be able to modify only the adapters anymore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f78e5a-9ea0-412c-8080-c463a3be6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()    # PEFT: incorporates LoRA into base weights\n",
    "merged_dir = f\"./{FINE_TUNED_MODEL_NAME}-merged\"\n",
    "merged_model.save_pretrained(merged_dir)\n",
    "tokenizer.save_pretrained(merged_dir)\n",
    "print(\"âœ“ Merged model saved to:\", merged_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
